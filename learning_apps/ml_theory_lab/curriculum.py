"""
Curriculum: Understanding Machine Learning (Shalev-Shwartz & Ben-David).
PAC learning, VC dimension, generalization, stability, Rademacher complexity. Theory-focused; no code required for concepts.
"""
from typing import Dict, Any, List

LEVELS = ["basics", "intermediate", "advanced", "expert"]

BOOKS = [
    {"id": "pac", "name": "PAC Learning", "short": "PAC", "color": "#2563eb"},
    {"id": "vc", "name": "VC Dimension & Generalization", "short": "VC & Gen", "color": "#059669"},
    {"id": "stability", "name": "Stability & Rademacher", "short": "Stability", "color": "#7c3aed"},
    {"id": "bounds", "name": "Generalization Bounds", "short": "Bounds", "color": "#dc2626"},
]

CURRICULUM: List[Dict[str, Any]] = [
    # PAC Learning Foundations (6 items)
    {"id": "theory_formal_model", "book_id": "pac", "level": "basics", "title": "Formal Model of Learning", "learn": "Domain X, label set Y, hypothesis class H, loss function ℓ. True risk L_D(h) = E[ℓ(h(x),y)]. Empirical risk L_S(h) = (1/m)∑ℓ(h(x_i),y_i). Goal: minimize true risk using finite sample.", "try_code": "# Formal framework: (X, Y, H, ℓ, D)\n# Example: Binary classification\nX = 'R^d'  # Domain\nY = {0, 1}  # Binary labels\nH = 'Linear classifiers'  # Hypothesis class\nloss = '0-1 loss'  # ℓ(ŷ, y) = 1[ŷ ≠ y]\nD = 'Unknown distribution'\n# True risk: L_D(h) = P_{(x,y)~D}[h(x) ≠ y]\n# Empirical risk: L_S(h) = (1/m)∑_{i=1}^m 1[h(x_i) ≠ y_i]", "try_demo": None, "prerequisites": []},
    {"id": "theory_erm", "book_id": "pac", "level": "basics", "title": "Empirical Risk Minimization (ERM)", "learn": "ERM: choose h ∈ H that minimizes empirical risk L_S(h). Assumption: H contains good hypothesis. Inductive bias = choice of H. Overfitting risk: small L_S but large L_D.", "try_code": "# ERM paradigm\ndef ERM(H, S):\n    # S = training set [(x_1,y_1), ..., (x_m,y_m)]\n    h_erm = None; min_loss = float('inf')\n    for h in H:\n        empirical_loss = sum(loss(h(x), y) for x, y in S) / len(S)\n        if empirical_loss < min_loss: min_loss = empirical_loss; h_erm = h\n    return h_erm\n# Key question: When does L_S(h) approximate L_D(h)?", "try_demo": None, "prerequisites": ["theory_formal_model"]},
    {"id": "theory_pac", "book_id": "pac", "level": "intermediate", "title": "Probably Approximately Correct (PAC)", "learn": "H is (ε,δ)-PAC learnable if ∃ algorithm A, sample complexity m_H(ε,δ) s.t. ∀D, with prob ≥1-δ over S~D^m (m≥m_H), L_D(A(S)) ≤ min_{h∈H} L_D(h) + ε. Sample complexity: how many examples for (ε,δ)-guarantee.", "try_code": "# PAC learning example: Finite H\n# Sample complexity for finite H (realizable case): m_H(ε, δ) ≤ ceil(ln(|H|/δ) / ε)\nimport math\ndef pac_sample_complexity_finite(H_size, epsilon, delta): return math.ceil(math.log(H_size / delta) / epsilon)\n# Example: |H| = 1000, ε = 0.1, δ = 0.05\nH_size = 1000; epsilon = 0.1; delta = 0.05\nm = pac_sample_complexity_finite(H_size, epsilon, delta)\nprint(f'PAC sample complexity: {m} examples')\n# Need ~100 examples to be 0.1-accurate with 95% confidence", "try_demo": "theory_pac", "prerequisites": ["theory_erm"]},
    {"id": "theory_realizable", "book_id": "pac", "level": "intermediate", "title": "Realizable vs Agnostic PAC", "learn": "Realizable: ∃h*∈H with L_D(h*)=0. Agnostic: no assumption, learn best in class. Agnostic harder: need more samples, L_D(h) ≤ min_{h'∈H}L_D(h') + ε. Real-world usually agnostic.", "try_code": "# Realizable PAC: assumes perfect hypothesis exists, m_H(ε, δ) = O(log(|H|/δ) / ε)\n# Agnostic PAC: no realizability assumption, m_H(ε, δ) = O(log(|H|/δ) / ε²)  # Note: ε² instead of ε!\nimport math\ndef sample_complexity_comparison(H_size, epsilon, delta):\n    realizable = math.log(H_size / delta) / epsilon\n    agnostic = math.log(H_size / delta) / (epsilon ** 2)\n    return realizable, agnostic\nreal, agn = sample_complexity_comparison(1000, 0.1, 0.05)\nprint(f'Realizable: {real:.0f}, Agnostic: {agn:.0f}')\n# Agnostic needs O(1/ε²) vs realizable O(1/ε)", "try_demo": None, "prerequisites": ["theory_pac"]},
    {"id": "theory_no_free_lunch", "book_id": "pac", "level": "intermediate", "title": "No Free Lunch Theorem", "learn": "No universal learner: for any algorithm A, ∃ distribution D where A fails. Inductive bias (restricting H) is necessary. Different H's suit different problems. Tradeoff: expressiveness vs sample complexity.", "try_code": "# No Free Lunch intuition: If H is too rich (e.g., all functions X→Y), then cannot learn from finite samples\n# Example: Consider all possible functions on domain X. For |X| = n, |Y| = 2, there are 2^n functions. Cannot distinguish from finite sample which is correct\n# Solution: Restrict H based on prior knowledge: Linear classifiers, Decision trees of bounded depth, Neural nets with specific architecture\n# Key insight: Learning = inductive bias + data", "try_demo": None, "prerequisites": ["theory_realizable"]},
    {"id": "theory_agnostic", "book_id": "pac", "level": "advanced", "title": "Agnostic PAC Learning", "learn": "No realizable assumption: best in class has error L*=min_{h∈H}L_D(h). Learner finds h with L_D(h)≤L*+ε with prob≥1-δ. Requires m=Ω(VC(H)/ε² + log(1/δ)/ε²) samples. Fundamental regime for practical ML.", "try_code": "# Agnostic PAC sample complexity bound: m ≥ C · (VC(H)/ε² + log(1/δ)/ε²) where C is universal constant\nimport math\ndef agnostic_pac_bound(vc_dim, epsilon, delta, C=32):\n    term1 = vc_dim / (epsilon ** 2); term2 = math.log(1 / delta) / (epsilon ** 2)\n    return int(math.ceil(C * (term1 + term2)))\n# Example: VC = 10, ε = 0.1, δ = 0.05\nvc = 10; eps = 0.1; delta = 0.05\nm = agnostic_pac_bound(vc, eps, delta)\nprint(f'Agnostic PAC needs ≥{m} samples')\n# Key: Sample complexity depends on VC dimension!", "try_demo": "theory_pac", "prerequisites": ["theory_realizable"]},
    
    # VC Dimension & Generalization (7 items)
    {"id": "theory_shattering", "book_id": "vc", "level": "basics", "title": "Shattering and VC Dimension", "learn": "H shatters set C if H realizes all 2^|C| labelings. VC(H) = max size of set H can shatter. Measures expressiveness. Examples: Linear classifiers in R^d have VC(H)=d+1. Intervals on R have VC=2.", "try_code": "# VC dimension examples:\n# 1. Intervals [a,b] on real line: Can shatter 2 points? Yes. Can shatter 3 points? No: cannot get x1=+, x2=-, x3=+. VC = 2\n# 2. Linear classifiers in R^2: w·x + b ≥ 0. Can shatter 3 points (not collinear)? Yes! Can shatter 4 points? No. VC = 3 (d+1 for R^d)\n# 3. Axis-aligned rectangles in R^2: VC = 4\n# Key: VC finite ⟹ learnable (Fundamental Theorem)", "try_demo": None, "prerequisites": []},
    {"id": "theory_vc", "book_id": "vc", "level": "intermediate", "title": "VC Dimension Theory", "learn": "VC(H)=d means: (1) ∃ size-d set H shatters, (2) no size-(d+1) set shattered. Key results: VC finite ⟺ PAC learnable. Sample complexity m=O(VC/ε² + log(1/δ)/ε²). Rich class ⟹ large VC ⟹ need more data.", "try_code": "import math\ndef vc_sample_bound(vc_dim, epsilon, delta): return int(math.ceil((8 / epsilon**2) * (vc_dim * math.log(13 / epsilon) + math.log(4 / delta))))\n# Example comparisons:\nfor vc in [1, 10, 100, 1000]:\n    m = vc_sample_bound(vc, 0.1, 0.05)\n    print(f'VC={vc:4d} → need ≥{m:6d} samples')\n# Output shows: Higher VC ⟹ more samples needed. This is the bias-complexity tradeoff!", "try_demo": "theory_vc", "prerequisites": ["theory_shattering"]},
    {"id": "theory_fundamental_thm", "book_id": "vc", "level": "advanced", "title": "Fundamental Theorem of Learning", "learn": "H is (agnostic) PAC learnable ⟺ VC(H) is finite. Sample complexity m_H(ε,δ)=Θ(VC(H)/ε² + log(1/δ)/ε²). Characterizes learnability completely. One of deepest results in ML theory.", "try_code": "# Fundamental Theorem of Statistical Learning: For binary classification with 0-1 loss:\n# PAC Learnable ⟺ VC(H) < ∞\n# Moreover: Lower bound: m ≥ Ω(VC(H)/ε + log(1/δ)/ε), Upper bound: m ≤ O((VC(H) + log(1/δ))/ε²)\n# Implications: 1. VC dimension fully characterizes learnability, 2. Cannot learn with infinite VC dimension, 3. Sample complexity polynomial in VC, 1/ε, log(1/δ)\n# Example: Neural networks - Perceptron in R^d: VC = d+1 (finite → learnable), All Boolean functions: VC = 2^n (infinite → not learnable)", "try_demo": None, "prerequisites": ["theory_vc"]},
    {"id": "theory_gen", "book_id": "vc", "level": "advanced", "title": "Generalization Bounds (VC-based)", "learn": "With prob ≥1-δ: L_D(h) ≤ L_S(h) + O(√(VC(H)/m) + √(log(1/δ)/m)). Uniform convergence over H. Bounds gap between empirical and true risk. Validates ERM principle.", "try_code": "import math\ndef vc_generalization_bound(vc_dim, m, delta):\n    term1 = (4 * vc_dim) / m; term2 = (2 * math.log(2 / delta)) / m\n    epsilon_gen = math.sqrt(term1 + term2)\n    return epsilon_gen\n# Example: How does bound shrink with more data?\nvc = 10; delta = 0.05\nfor m in [100, 1000, 10000, 100000]:\n    bound = vc_generalization_bound(vc, m, delta)\n    print(f'm={m:6d}: generalization gap ≤ {bound:.4f}')\n# Output shows: gap ~ O(1/√m). Need 4x data to halve the gap!", "try_demo": "theory_gen_bound", "prerequisites": ["theory_fundamental_thm"]},
    {"id": "theory_vc_computation", "book_id": "vc", "level": "advanced", "title": "Computing VC Dimension", "learn": "Examples: Linear classifiers R^d: VC=d+1. Decision trees depth k: VC=O(2^k). Neural nets: VC≥W (W=weights). Unions/intersections: VC(H₁∪H₂)≤VC(H₁)+VC(H₂)+1. Polynomial growth ⟹ learnable.", "try_code": "# VC dimension examples and computations:\nimport math\nlinear_vc = lambda d: d + 1  # Linear threshold in R^d\ntree_vc = lambda depth: int((2**(depth+1) - 1) * math.log2(2**(depth+1)))  # Decision tree depth k\nnn_vc_lower = lambda W: W  # Neural network with W weights: VC ≥ W\nnn_vc_upper = lambda W: int(W**2 * math.log2(max(W, 2)))  # Upper bound: VC ≤ O(W² log W)\nprint(f'Linear R^10: VC = {linear_vc(10)}'); print(f'Tree depth 5: VC ≈ {tree_vc(5)}'); print(f'NN 100 weights: {nn_vc_lower(100)} ≤ VC ≤ {nn_vc_upper(100)}')\n# Composition rules: VC(H1 ∪ H2) ≤ VC(H1) + VC(H2) + 1, VC(H1 ∩ H2) ≤ VC(H1) + VC(H2)\nunion_vc_bound = lambda vc1, vc2: vc1 + vc2 + 1", "try_demo": None, "prerequisites": ["theory_vc"]},
    {"id": "theory_structural_risk", "book_id": "vc", "level": "advanced", "title": "Structural Risk Minimization", "learn": "SRM: Choose H from nested sequence H₁⊆H₂⊆... trading bias vs complexity. Minimize L_S(h) + penalty(VC(H_i)). Occam's Razor principle. Used in model selection, regularization. Theory behind cross-validation.", "try_code": "# Structural Risk Minimization (SRM): Select from nested hypothesis classes\nimport math\ndef srm_bound(empirical_risk, vc_dim, m, delta_i):\n    complexity_penalty = math.sqrt((vc_dim * math.log(m / vc_dim) + math.log(1 / delta_i)) / m)\n    return empirical_risk + complexity_penalty\nm = 1000; models = [('Linear', 0.15, 5), ('Poly degree 3', 0.10, 20), ('Poly degree 5', 0.08, 56), ('Deep NN', 0.05, 500)]\nfor i, (name, emp_risk, vc) in enumerate(models):\n    delta_i = 0.05 / len(models); bound = srm_bound(emp_risk, vc, m, delta_i)\n    print(f'{name:15s}: emp={emp_risk:.3f}, VC={vc:3d}, bound={bound:.3f}')\n# SRM selects model minimizing bound (not just empirical risk!)", "try_demo": None, "prerequisites": ["theory_gen"]},
    {"id": "theory_bias_complexity", "book_id": "vc", "level": "advanced", "title": "Bias-Complexity Tradeoff", "learn": "True error = approximation error (bias) + estimation error (complexity). Simple H: low estimation error (few params), high bias (limited expressiveness). Rich H: low bias, high estimation error. Optimal H balances both.", "try_code": "# Bias-Complexity Tradeoff Decomposition: L_D(h_S) = approximation + estimation + optimization\n# 1. Approximation error (bias): L* = min_{h∈H} L_D(h). How well can H represent true function?\n# 2. Estimation error (complexity): L_D(h_S) - L*. Gap due to finite sample S. Grows with VC(H), shrinks with m\n# 3. Optimization error: If cannot find exact ERM (NP-hard)\nimport numpy as np\ndef bias_complexity_curve(vc_range, m=1000):\n    bias = [10 / (vc + 1) for vc in vc_range]; complexity = [np.sqrt(vc / m) for vc in vc_range]\n    total = [b + c for b, c in zip(bias, complexity)]; return bias, complexity, total\nvc_range = range(1, 100); bias, complexity, total = bias_complexity_curve(vc_range)\noptimal_vc = vc_range[np.argmin(total)]; print(f'Optimal VC dimension: {optimal_vc}')", "try_demo": "theory_bias_var", "prerequisites": ["theory_structural_risk"]},
    
    # Rademacher Complexity & Stability (6 items)
    {"id": "theory_rademacher_def", "book_id": "stability", "level": "advanced", "title": "Rademacher Complexity Definition", "learn": "R_m(H) = E_σ[sup_{h∈H} (1/m)∑σᵢh(zᵢ)] where σᵢ∈{±1} uniform. Measures how well H correlates with random noise. Finer than VC: data-dependent. Generalization: E[L_D(h)] ≤ L_S(h) + 2R_m(H) + O(√(log(1/δ)/m)).", "try_code": "# Rademacher complexity intuition: If H can fit random labels σ, it's too complex\nimport numpy as np\ndef empirical_rademacher(H, S):\n    m = len(S); n_trials = 1000; max_correlations = []\n    for _ in range(n_trials):\n        sigma = np.random.choice([-1, 1], size=m)\n        max_corr = max(np.mean([sigma[i] * h(S[i][0]) for i in range(m)]) for h in H)\n        max_correlations.append(max_corr)\n    return np.mean(max_correlations)\n# Example: Linear functions on unit ball, R(H) ≈ O(√(d/m)). Key: Lower Rademacher → better generalization", "try_demo": "theory_rademacher", "prerequisites": []},
    {"id": "theory_rademacher", "book_id": "stability", "level": "expert", "title": "Rademacher Complexity Bounds", "learn": "R_m(H)=E_σ[sup_{h∈H}(1/m)∑σᵢh(zᵢ)]. Bounds: With prob≥1-δ, L_D(h)≤L_S(h)+2R_m(H)+O(√(log(1/δ)/m)). Tighter than VC for some classes. Examples: Linear functions: R_m≈O(√(d/m)), Neural nets: R_m depends on norms.", "try_code": "import math\ndef rademacher_gen_bound(emp_loss, rad_complexity, m, delta): return emp_loss + 2 * rad_complexity + 3 * math.sqrt(math.log(2/delta) / (2*m))\ndef linear_rademacher(d, B, W, m): return (B * W * math.sqrt(d)) / math.sqrt(m)  # R_m ≈ (B*W)/√m where B=data norm, W=weight norm\nd = 100; B = 1.0; W = 1.0; m = 1000\nrad = linear_rademacher(d, B, W, m); emp_loss = 0.1; bound = rademacher_gen_bound(emp_loss, rad, m, 0.05)\nprint(f'Rademacher complexity: {rad:.4f}'); print(f'Generalization bound: {bound:.4f}')\n# Note: Bound tightens with more data (R_m ~ 1/√m)", "try_demo": "theory_rademacher", "prerequisites": ["theory_rademacher_def"]},
    {"id": "theory_stability_def", "book_id": "stability", "level": "advanced", "title": "Algorithmic Stability", "learn": "Algorithm A is β-uniformly stable if |ℓ(A(S),z) - ℓ(A(S^i),z)| ≤ β ∀z, where S^i differs from S in one example. Stable algorithms generalize: E[L_D(A(S))] ≤ L_S(A(S)) + β + O(√(log(1/δ)/m)).", "try_code": "# Stability: Changing one training example changes loss by ≤ β\ndef check_stability(algorithm, S, test_point):\n    h = algorithm(S); loss_full = compute_loss(h, test_point); max_diff = 0\n    for i in range(len(S)):\n        S_minus_i = S[:i] + S[i+1:]; h_minus_i = algorithm(S_minus_i); loss_minus_i = compute_loss(h_minus_i, test_point)\n        diff = abs(loss_full - loss_minus_i); max_diff = max(max_diff, diff)\n    return max_diff  # This is β\n# Examples: 1. SGD with step size η: β = O(η) (stable if η small), 2. SVM with regularization λ: β = O(1/(λm)) (stable), 3. Memorization (nearest neighbor): β = ∞ (unstable!)", "try_demo": None, "prerequisites": []},
    {"id": "theory_stability", "book_id": "stability", "level": "advanced", "title": "Stability and Generalization", "learn": "Uniform stability: changing one sample changes loss by ≤β. Stable algorithms generalize well even with large VC. Examples: Ridge regression (β=O(1/(λm))), SGD with small steps. Alternative to VC/Rademacher.", "try_code": "import math\ndef stability_generalization_bound(beta, m, delta): return beta + (4 * m * beta + 1) * math.sqrt(math.log(1/delta) / (2*m))\ndef ridge_stability(lambda_param, m, L=1.0): return (L ** 2) / (2 * lambda_param * m)  # β = L² / (2 * λ * m) for L-Lipschitz loss\nm = 1000; lambda_param = 0.01\nbeta = ridge_stability(lambda_param, m); gen_gap = stability_generalization_bound(beta, m, 0.05)\nprint(f'Stability parameter β: {beta:.6f}'); print(f'Generalization gap bound: {gen_gap:.4f}')\n# Key insight: Regularization → stability → generalization. This explains why λ ↑ improves test performance (up to a point)", "try_demo": None, "prerequisites": ["theory_stability_def"]},
    {"id": "theory_sgd_stability", "book_id": "stability", "level": "expert", "title": "SGD and Stability Analysis", "learn": "SGD with step size η: uniform stability β=O(ηT/m) where T=iterations. Small η + early stopping ⟹ stable ⟹ generalize. Implicit regularization: SGD prefers flat minima. Explains why deep learning generalizes despite overparameterization.", "try_code": "import math\ndef sgd_stability_bound(eta, T, m, L=1.0, smoothness=1.0): return (2 * L**2 * eta * T) / m  # SGD uniform stability (Hardt et al., 2016)\ndef sgd_generalization(beta, m, delta): return beta + math.sqrt(math.log(1/delta) / (2*m))\nm = 10000; T = 1000; eta = 0.01\nbeta = sgd_stability_bound(eta, T, m); gen_gap = sgd_generalization(beta, m, 0.05)\nprint(f'SGD stability: β = {beta:.6f}'); print(f'Expected generalization gap: {gen_gap:.4f}')\n# Key insights: 1. Smaller η → more stable → better generalization, 2. Early stopping (smaller T) → more stable, 3. More data (larger m) → more stable, 4. This explains implicit regularization of SGD!", "try_demo": None, "prerequisites": ["theory_stability"]},
    {"id": "theory_compression", "book_id": "stability", "level": "expert", "title": "Compression Bounds", "learn": "If hypothesis h can be represented using k examples from S, then generalization gap = O(k/m). Compression ⟹ generalization. Examples: SVM support vectors, decision tree leaves. Alternative view of Occam's Razor.", "try_code": "import math\ndef compression_bound(k, m, delta):\n    if k >= m: return float('inf')\n    term1 = math.sqrt((k * math.log(m / k)) / m); term2 = math.sqrt(math.log(1 / delta) / m)\n    return 2 * term1 + term2\nm = 1000\nfor k in [10, 50, 100, 500]:\n    bound = compression_bound(k, m, 0.05)\n    print(f'k={k:3d} support vectors: gen gap ≤ {bound:.4f}')\n# Insights: 1. Fewer support vectors → better bound, 2. Large margin → fewer SVs → generalization, 3. This justifies margin-based learning!", "try_demo": None, "prerequisites": ["theory_sgd_stability"]},
    
    # Advanced Topics (9 items)
    {"id": "theory_boosting", "book_id": "bounds", "level": "advanced", "title": "Boosting Theory", "learn": "AdaBoost combines weak learners (>50% accuracy) into strong learner. Training error decays exponentially: ε_t ≤ exp(-2t∑γ_t²). Generalization via margins: large margin ⟹ good generalization despite no explicit regularization.", "try_code": "import numpy as np\ndef adaboost_training_error(T, gamma_min): return np.exp(-2 * T * gamma_min**2)  # If each weak learner has edge γ_t ≥ γ_min > 0\ngamma = 0.05\nfor T in [10, 50, 100, 500]:\n    err = adaboost_training_error(T, gamma)\n    print(f'T={T:3d} rounds: training error ≤ {err:.6f}')\n# Generalization: margin theory (Schapire et al., 1998). Key: Large margin → good generalization (even if many weak learners!)", "try_demo": None, "prerequisites": []},
    {"id": "theory_online_learning", "book_id": "bounds", "level": "advanced", "title": "Online Learning and Regret", "learn": "Online: see x_t, predict ŷ_t, observe y_t, incur loss. Regret: R_T = ∑ℓ(ŷ_t,y_t) - min_{h∈H}∑ℓ(h(x_t),y_t). Goal: sublinear regret R_T = o(T). Multiplicative weights, gradient descent achieve R_T = O(√T).", "try_code": "import numpy as np, math\nclass OnlineGD:\n    def __init__(self, d, eta): self.w = np.zeros(d); self.eta = eta; self.losses = []\n    def predict(self, x): return np.dot(self.w, x)\n    def update(self, x, y, loss_grad): self.w = self.w - self.eta * loss_grad; norm = np.linalg.norm(self.w); self.w = self.w / norm if norm > 1.0 else self.w\ndef ogd_regret_bound(T, D=1.0, L=1.0): eta_opt = D / (L * math.sqrt(T)); regret = D * L * math.sqrt(T); return regret, eta_opt\nfor T in [100, 1000, 10000]: regret, eta = ogd_regret_bound(T); print(f'T={T:5d}: Regret ≤ {regret:.2f}, use η={eta:.4f}')", "try_demo": None, "prerequisites": []},
    {"id": "theory_kernel_theory", "book_id": "bounds", "level": "advanced", "title": "Kernel Methods and Representer Theorem", "learn": "Kernel k(x,x')=⟨φ(x),φ(x')⟩ allows learning in high/infinite dim. Representer theorem: solution h*=∑α_i k(x_i,·). Generalization via kernel alignment, margin bounds. Examples: RBF, polynomial kernels.", "try_code": "import numpy as np\nclass KernelRidgeRegression:\n    def __init__(self, kernel, lambda_reg): self.kernel = kernel; self.lambda_reg = lambda_reg; self.alpha = None; self.X_train = None\n    def fit(self, X, y): m = len(X); K = np.array([[self.kernel(X[i], X[j]) for j in range(m)] for i in range(m)]); self.alpha = np.linalg.solve(K + self.lambda_reg * np.eye(m), y); self.X_train = X\n    def predict(self, x): return sum(self.alpha[i] * self.kernel(self.X_train[i], x) for i in range(len(self.X_train)))\ndef rbf_kernel(x1, x2, gamma=1.0): return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)", "try_demo": None, "prerequisites": []},
    {"id": "theory_neural_net_theory", "book_id": "bounds", "level": "expert", "title": "Neural Network Generalization", "learn": "Overparameterized NNs (W≫m) can fit noise yet generalize. Explanations: (1) Implicit regularization of SGD prefers flat minima, (2) Norm-based bounds: generalization via ||W||, (3) Compression via pruning. Active research area.", "try_code": "import math\ndef nn_norm_bound(weights_product, margin, depth, m, delta): term1 = (weights_product * math.sqrt(depth)) / (margin * math.sqrt(m)); term2 = math.sqrt(math.log(1/delta) / m); return term1 + term2\nW1_norm = 2.0; W2_norm = 2.0; W3_norm = 2.0; product = W1_norm * W2_norm * W3_norm; margin = 0.1; depth = 3; m = 10000\nbound = nn_norm_bound(product, margin, depth, m, 0.05); print(f'NN generalization bound: {bound:.4f}')\n# Key insights: Smaller weight norms → better bound. This justifies weight decay / L2 regularization! Larger margin → better bound. Bound is independent of width (overparameterization OK!)", "try_demo": None, "prerequisites": []},
    {"id": "theory_double_descent", "book_id": "bounds", "level": "expert", "title": "Double Descent Phenomenon", "learn": "Risk curve non-monotonic: decreases, peaks at interpolation threshold (W=m), then decreases again. Classical regime (W<m): bias-variance tradeoff. Modern regime (W>m): implicit regularization. Challenges traditional theory.", "try_code": "# Double Descent: Risk as function of model complexity. Classical U-curve vs Double Descent (modern ML). Three regimes:\n# 1. Underparameterized (W << m): Classical bias-variance\n# 2. Critically parameterized (W ≈ m): Interpolates with high norm\n# 3. Overparameterized (W >> m): Interpolates with low norm (implicit reg)\nimport numpy as np\ndef double_descent_curve(complexity_range, m=100):\n    risk = []\n    for W in complexity_range:\n        if W < m * 0.8: r = 1.0 / W + (W / m) * 0.1  # Underparameterized: U-curve\n        elif W < m * 1.2: r = 2.0 + 5 * abs(W - m) / m  # Interpolation threshold: PEAK\n        else: r = 1.0 + 10.0 / (W - m)  # Overparameterized: decreasing\n        risk.append(r)\n    return risk", "try_demo": None, "prerequisites": ["theory_neural_net_theory"]},
    {"id": "theory_pac_bayes", "book_id": "bounds", "level": "expert", "title": "PAC-Bayesian Theory", "learn": "Bayesian learning theory: prior P over H, posterior Q. PAC-Bayes bound: KL(Q||P) ≤ (1/λ)∑ℓ(h,z) + O(log(m/δ)/λm). Tighter for stochastic predictors. Applications: deep learning generalization, compression.", "try_code": "import math\ndef pac_bayes_bound(empirical_loss, kl_divergence, m, delta, lambda_param=1.0): kl_term = kl_divergence + math.log(2 * math.sqrt(m) / delta); penalty = math.sqrt(kl_term / (2 * m)); return empirical_loss + penalty\ndef kl_gaussian(mu_post, sigma_post, mu_prior, sigma_prior, d): return 0.5 * d * (math.log(sigma_prior**2 / sigma_post**2) + (sigma_post**2 + (mu_post - mu_prior)**2) / sigma_prior**2 - 1)\nd = 100; kl = kl_gaussian(0.1, 0.5, 0.0, 1.0, d); emp_loss = 0.15; m = 1000\nbound = pac_bayes_bound(emp_loss, kl, m, 0.05); print(f'KL(Q||P) = {kl:.2f}'); print(f'PAC-Bayes bound: {bound:.4f}')", "try_demo": None, "prerequisites": []},
    {"id": "theory_label_complexity", "book_id": "bounds", "level": "advanced", "title": "Label Complexity and Active Learning", "learn": "Active learning: learner chooses which examples to label. Can achieve exp(VC) reduction in label complexity vs passive. Query strategies: uncertainty sampling, QBC. Theory: disagreement coefficient bounds improvement.", "try_code": "import math\ndef label_complexity_passive(vc, epsilon, delta): return (vc / epsilon**2) + (math.log(1/delta) / epsilon**2)\ndef label_complexity_active(vc, epsilon, delta, theta): return theta * vc * math.log(1/epsilon) + math.log(1/delta) / epsilon\nvc = 10; epsilon = 0.1; delta = 0.05\npassive = label_complexity_passive(vc, epsilon, delta); active_best = label_complexity_active(vc, epsilon, delta, theta=1); active_worst = label_complexity_active(vc, epsilon, delta, theta=vc)\nprint(f'Passive: {passive:.0f} labels'); print(f'Active (best case θ=1): {active_best:.0f} labels'); print(f'Speedup: {passive/active_best:.1f}x')", "try_demo": None, "prerequisites": []},
    {"id": "theory_privacy", "book_id": "bounds", "level": "expert", "title": "Differential Privacy and Learning", "learn": "(ε,δ)-differential privacy: Pr[A(D)]≤e^ε Pr[A(D')]+δ for neighboring datasets. Private learning possible via output perturbation or gradient noise. Cost: O(1/εn) sample complexity overhead. Trade privacy for accuracy.", "try_code": "import numpy as np\ndef private_mean(data, epsilon, sensitivity=1.0): mean = np.mean(data); noise = np.random.laplace(0, sensitivity / epsilon); return mean + noise\ndef dp_sgd_step(gradient, epsilon, C=1.0): norm = np.linalg.norm(gradient); gradient = gradient * (C / norm) if norm > C else gradient; noise = np.random.normal(0, C / epsilon, size=gradient.shape); return gradient + noise\ndef dp_sample_complexity(vc, epsilon_acc, delta_acc, epsilon_priv): return (vc / epsilon_acc**2) + (vc / (epsilon_priv * epsilon_acc))\nfor eps_priv in [10, 1, 0.1]: m = dp_sample_complexity(10, 0.1, 0.05, eps_priv); print(f'ε_priv={eps_priv:4.1f}: m ≈ {m:.0f}')", "try_demo": None, "prerequisites": []},
    {"id": "theory_multiclass", "book_id": "bounds", "level": "advanced", "title": "Multiclass Learning Theory", "learn": "K classes: Natarajan dimension extends VC. Sample complexity m=O(d·log(K)/ε²) where d=Natarajan dim. Error-correcting codes, one-vs-all, tree-based. Generalization via margin for output code.", "try_code": "import math\ndef natarajan_sample_bound(d_nat, K, epsilon, delta): C = 32; term1 = (d_nat * math.log(K)) / (epsilon ** 2); term2 = math.log(1 / delta) / (epsilon ** 2); return int(math.ceil(C * (term1 + term2)))\nd = 100\nfor K in [2, 5, 10, 100]:\n    d_nat = (d + 1) * K; m = natarajan_sample_bound(d_nat, K, 0.1, 0.05)\n    print(f'K={K:3d} classes: need ≥{m:7d} samples')\n# Reduction strategies: 1. One-vs-all: K binary problems, 2. All-pairs: K(K-1)/2 binary, 3. Error-correcting output codes: log(K) binary, 4. Tree-based: log(K) depth", "try_demo": None, "prerequisites": []},
]


def get_curriculum(): return list(CURRICULUM)
def get_books(): return list(BOOKS)
def get_levels(): return list(LEVELS)
def get_by_book(book_id: str): return [c for c in CURRICULUM if c["book_id"] == book_id]
def get_by_level(level: str): return [c for c in CURRICULUM if c["level"] == level]
def get_item(item_id: str):
    for c in CURRICULUM:
        if c["id"] == item_id: return c
    return None
